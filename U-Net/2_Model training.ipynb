{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2ZH7lxdc2r"
   },
   "source": [
    "<h1> Wildebeest Detection using U-Net from VHR satellite images</h1>\n",
    "Code Author: Zijing Wu\n",
    "\n",
    "***The code is developed for educational project purposes.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44726,
     "status": "ok",
     "timestamp": 1701398746630,
     "user": {
      "displayName": "Zijing Wu",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "fSBzYbQo2E1o",
    "outputId": "8095f220-7852-4fe2-a822-065d1050e766"
   },
   "outputs": [],
   "source": [
    "#If you are using Google Colaboratory to run this code, please upload the whole folder to your Google Drive, and run this cell install the requirements.\n",
    "\n",
    "#connect to the google drive if you use Google Colaboratory\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "#tf.device('/device:GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device('/device:GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the GPU colab assigns to you\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWyyFIrvrwXd"
   },
   "source": [
    "# Load libraries and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlLAc-0H50f2"
   },
   "source": [
    "##Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29493,
     "status": "ok",
     "timestamp": 1701398777159,
     "user": {
      "displayName": "Zijing Wu",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "2QmzBZgmeTNI",
    "outputId": "62ead3af-2623-4dfb-b2fa-db6079e3d4e4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "\n",
    "import numpy as np               # numerical array manipulation\n",
    "#from tqdm import tqdm\n",
    "# import cv2\n",
    "import random\n",
    "from rasterio.windows import Window\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 10)\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3374,
     "status": "ok",
     "timestamp": 1701398780514,
     "user": {
      "displayName": "Zijing Wu",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "pn_fGBi4pC61",
    "outputId": "b2b9c235-f666-44f8-faa2-f240cde23825"
   },
   "outputs": [],
   "source": [
    "#set the sys path where the modules locates\n",
    "import sys\n",
    "sys.path.insert(0,\"core\")\n",
    "\n",
    "#If you are using Google Colaboratory, modify the path here\n",
    "#sys.path.insert(0,\"/content/drive/MyDrive/Colab/zijingwu-Satellite-based-monitoring-of-wildebeest/core\")\n",
    "from preprocess import *\n",
    "from data_generator import DataGenerator, SimpleDataGenerator\n",
    "\n",
    "from model import *\n",
    "\n",
    "from evaluation import *\n",
    "\n",
    "from visualization import *\n",
    "\n",
    "import importlib\n",
    "\n",
    "from predict import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D219v7fFxkcx"
   },
   "source": [
    "## Set data file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE=336\n",
    "INPUT_BANDS = [0,1,2]\n",
    "NUMBER_BANDS=len(INPUT_BANDS)\n",
    "\n",
    "Data_folder = \"/home/zijing/wildebeest/SampleData\"\n",
    "Folder = \"/home/zijing/wildebeest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of savinng the training data directory in a csv file\n",
    "\n",
    "year_list = [\"data_2009Aug\", \"data_2010Sep\", \"data_2013Aug\", \"data_2015Jul\", \"data_2018Aug\", \"data_2020Oct\"]\n",
    "\n",
    "# The images are sometimes not in the size of patch_size x patch_size.\n",
    "# Use pre_directory() to further “crop” them by recording the window dimension info in the directory.\n",
    "# The DataGenrator class is defined for this type of data storage.\n",
    "# If your data is already cropped properly, then please use SimpleDataGenerator instead.\n",
    "\n",
    "for year in year_list:\n",
    "    image_path = Data_folder+'/'+year+'/'+'3_Train_test/train/image'\n",
    "    label_path = Data_folder+'/'+year+'/'+'3_Train_test/train/mask'\n",
    "    head = year\n",
    "    out_path = os.path.join(Data_folder, 'update_train2023_match2023_4_dict_train_filelinks.csv')\n",
    "    prep_directory(head, image_path, label_path, out_path, bandorder=\"123\", stretch=0, stride=PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vp3QXVzrcwt"
   },
   "source": [
    "# Model generation (U-Net) and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atE9hVuW8kq0"
   },
   "source": [
    "## K-fold splitting ensemble\n",
    "To achieve a more robust and reliable model, we adopted K-fold spliting to create an ensemble model.\n",
    "We split the training dataset into k folds. We use 1 fold as the validation dataset during training, and the remaining k-1 folds as the training dataset. This way we will have k models with each model learning different variations of the dataset.\n",
    "Then the predictions of all the k models are averaged to get the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#set the number of folds -- k\n",
    "num_folds = 5\n",
    "# Define per-fold score containers\n",
    "fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "\n",
    "Val_precision_per_fold = []\n",
    "Val_recall_per_fold = []\n",
    "Val_f1_per_fold = []\n",
    "\n",
    "Test_precision_per_fold = []\n",
    "Test_recall_per_fold = []\n",
    "Test_f1_per_fold = []\n",
    "\n",
    "\n",
    "df_ori = pd.read_csv(os.path.join(Data_folder, 'update_dict_comb_filelinks.csv'))\n",
    "df2023 = pd.read_csv(os.path.join(Data_folder, 'update_match2023_dict_comb_filelinks.csv'))\n",
    "df2023_4 = pd.read_csv(os.path.join(Data_folder, 'update_match2023_4_dict_comb_filelinks.csv'))\n",
    "\n",
    "train_df = pd.concat([df_ori, df2023, df2023_4]).reset_index(drop=True)\n",
    "\n",
    "train_df = train_df[(train_df['Window_width']>=335) & (train_df['Window_height']>=335)].reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state = 3)\n",
    "#print(Xtrain)\n",
    "\n",
    "#split the dataset into k folds, save the index of training and validation data\n",
    "split = []\n",
    "for train, val in kf.split(train_df.index):\n",
    "    com = {'train': train, 'val': val}\n",
    "    split.append(com)\n",
    "    print(com['train'])\n",
    "    print(com['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA-0yPELA0f1"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1658470114913,
     "user": {
      "displayName": "Juli Baker",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "SqyliTcaNLD6",
    "outputId": "0127afa1-e1e1-46e5-c254-a79905f487eb"
   },
   "outputs": [],
   "source": [
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "weight_path = os.path.join(Folder, \"tmp/test\")\n",
    "if not os.path.exists(weight_path):\n",
    "  os.makedirs(weight_path)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='loss', factor=0.33,\n",
    "                                   patience=10, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=4, min_lr=1e-16)\n",
    "\n",
    "early = EarlyStopping(monitor=\"loss\", mode=\"min\", verbose=2, patience=20)\n",
    "\n",
    "#Use this directory if you are using Google Colaboratory\n",
    "log_dir = Folder+\"/tmp/logs/UNet\"\n",
    "# log_dir=Folder+\"/tmp/logs/UNet\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_grads=False, write_images=False, \n",
    "                          embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir /home/zijing/wildebeest/tmp/logs/UNet #put the absolute path here\n",
    "# %tensorboard --logdir /content/drive/MyDrive/Colab/Wildebeest-UNet/tmp/logs/UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2938634,
     "status": "error",
     "timestamp": 1658420528065,
     "user": {
      "displayName": "Juli Baker",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "LGSXYVisiSMh",
    "outputId": "1593e8b0-9ed7-453d-8b03-446d3eb4c57d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#k-fold ensemble model training\n",
    "fold_no = 1\n",
    "i = fold_no - 1\n",
    "\n",
    "NUMBER_EPOCHS = 60\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "#set weight for tversky loss in core/model.py\n",
    "weight_set = 0.8\n",
    "\n",
    "lr_set = 1.0e-04\n",
    "drop_set = 0\n",
    "\n",
    "# The images are sometimes not in the size of patch_size x patch_size.\n",
    "# Use pre_directory() to further “crop” them by recording the window dimension info in the directory.\n",
    "# The DataGenrator class is defined for this type of data storage.\n",
    "# If your data is already cropped properly, then please use SimpleDataGenerator instead.\n",
    "\n",
    "train_params = {'patchsize': PATCH_SIZE,\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'input_image_channel': [0,1,2],\n",
    "          'shuffle': True,\n",
    "         'augment': True,\n",
    "        'folder': \"/home/zijing/wildebeest/SampleData\"}\n",
    "val_params = {'patchsize': PATCH_SIZE,\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'input_image_channel': [0,1,2],\n",
    "          'shuffle': True,\n",
    "         'augment': False,\n",
    "        'folder': \"/home/zijing/wildebeest/SampleData\"}\n",
    "\n",
    "\n",
    "\n",
    "while i < num_folds:\n",
    "   \n",
    "    train_data = train_df.iloc[split[i]['train']].reset_index()\n",
    "    val_data = train_df.iloc[split[i]['val']].reset_index()\n",
    "    # train_df = df.reset_index()\n",
    "\n",
    "    training_generator = DataGenerator(train_data,  **train_params)\n",
    "    validation_generator = DataGenerator(val_data,  **val_params)\n",
    "\n",
    "    \n",
    "    pretrained_weight_path = None  \n",
    "    model = unet(pretrained_weights=pretrained_weight_path, input_size = (PATCH_SIZE,PATCH_SIZE,NUMBER_BANDS),\n",
    "                 lr = lr_set, drop_out = drop_set)\n",
    "    model.summary()\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    # check_path = os.path.join(weight_path, 'weights.'+str(fold_no)+'_{epoch:02d}-{loss:.4f}.hdf5')\n",
    "    check_path = os.path.join(weight_path, 'weights.'+str(fold_no)+'_{epoch:02d}-{val_loss:.4f}.hdf5')    \n",
    "    checkpoint = ModelCheckpoint(check_path, monitor='val_loss', verbose=1,\n",
    "                                 save_best_only=True, mode='min', save_weights_only = True)\n",
    "    callbacks_list = [checkpoint, reduceLROnPlat, early, tensorboard] #reduceLROnPlat is not required with adaDelta\n",
    "\n",
    "    hist = model.fit(training_generator,\n",
    "                     epochs=NUMBER_EPOCHS,\n",
    "                     validation_data=validation_generator,\n",
    "                     callbacks=callbacks_list,\n",
    "                     verbose=1\n",
    "                     # use_multiprocessing=True,\n",
    "                     # workers=8\n",
    "                    )\n",
    "      # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    # plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    history = pd.DataFrame(hist.history)\n",
    "    history.to_csv(os.path.join(weight_path, 'hist_fold{}.csv'.format(fold_no)))\n",
    "\n",
    "    # find the best weights with lowest validation loss\n",
    "    hist_loss = [item for item in hist.history['val_loss'] if not(pd.isnull(item))]\n",
    "    if len(hist_loss) == 0:\n",
    "        break\n",
    "    best_loss = np.min(hist_loss)\n",
    "    best_epoch = hist.history['val_loss'].index(best_loss)+1\n",
    "\n",
    "    #add criteria of lowest_loss to determine if it is needed to retrain the model\n",
    "    #because sometimes the loss does not decrease at all, and it need to be retrained\n",
    "\n",
    "    if best_loss < 1:\n",
    "        #save the training history\n",
    "        history = pd.DataFrame(hist.history)\n",
    "        history.to_csv(os.path.join(weight_path, 'hist_fold{}.csv'.format(fold_no)))\n",
    "        best_path = os.path.join(weight_path, 'weights.{}_{:02d}-{:.4f}.hdf5'.format(fold_no,best_epoch,best_loss))\n",
    "        print(best_path)\n",
    "\n",
    "        model.load_weights(best_path)\n",
    "\n",
    "        #rename the best weights\n",
    "        os.rename(best_path,os.path.join(weight_path, 'best_weights_fold_{}.hdf5'.format(fold_no)))\n",
    "\n",
    "        del model\n",
    "        del hist\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "        i = i + 1\n",
    "\n",
    "    else:\n",
    "        print(\"The loss did not decrease significantly. Retrain this model...\")\n",
    "        del model\n",
    "        del hist\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y0VWfw3kCiq"
   },
   "source": [
    "#References\n",
    "***References***\n",
    "\n",
    "Ankit. (2020). ankitkariryaa/An-unexpectedly-large-count-of-trees-in-the-western-Sahara-and-Sahel: Paper version (v1.0.0). Zenodo. https://doi.org/10.5281/zenodo.3978185"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3Y0VWfw3kCiq"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
