{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4x9H2wJ1m7j"
   },
   "source": [
    "## Load libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101276,
     "status": "ok",
     "timestamp": 1706781768070,
     "user": {
      "displayName": "Zijing Wu",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "rHYQ6yZ5R-TL",
    "outputId": "2390b6e2-958d-45fe-b304-c07e9beab7dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the GPU colab assigns to you\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device('/device:GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13738,
     "status": "ok",
     "timestamp": 1706782085508,
     "user": {
      "displayName": "Zijing Wu",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "mW60uNlcTOd9",
    "outputId": "cba3dee4-0af9-43fd-c9f5-4fe53dca48df"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "#import rasterio.warp             # Reproject raster samples\n",
    "from rasterio import windows\n",
    "#import geopandas as gps\n",
    "#import PIL.Image\n",
    "#import PIL.ImageDraw\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 10)\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from matplotlib import pylab\n",
    "pylab.rcParams['figure.figsize'] = (15, 10)\n",
    "# !pip install ipython-autotime\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6356,
     "status": "ok",
     "timestamp": 1706782091827,
     "user": {
      "displayName": "Zijing Wu",
      "userId": "12694382704677250833"
     },
     "user_tz": -480
    },
    "id": "1hoOpPmE1hl4",
    "outputId": "7b9934d9-c845-438a-9a88-22486f61ce27"
   },
   "outputs": [],
   "source": [
    "#set the sys path where the modules locates\n",
    "import sys\n",
    "sys.path.insert(0,\"core\")\n",
    "\n",
    "#If you are using Google Colaboratory, modify the path here\n",
    "#sys.path.insert(0,\"/content/drive/MyDrive/Colab/zijingwu-Satellite-based-monitoring-of-wildebeest/core\")\n",
    "from preprocess import *\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "from model import *\n",
    "\n",
    "from evaluation import *\n",
    "\n",
    "from visualization import *\n",
    "\n",
    "import importlib\n",
    "\n",
    "from predict import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQxlg_ALTK3J"
   },
   "source": [
    "## Detect the wildebeest on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calculate the accuracy for each density class\n",
    "#Criteria: a predicted point is within a searching radius (in meter) of the ground truth point\n",
    "\n",
    "# data projection: prediction points match the image (4326) -> convert EPSG 32736; true points also 4326 -> convert EPSG 32736\n",
    "        \n",
    "        \n",
    "# annotation_path = \"/content/drive/MyDrive/Colab/zijingwu-Satellite-based-monitoring-of-wildebeest/SampleData/Test2023\"\n",
    "annotation_path = \"/home/zijing/wildebeest/Sample_test2023/WV3_2023_test_label\"\n",
    "\n",
    "low_grid = os.path.join(annotation_path, \"2023 test sample grid/Low_desnity_test_sample_grid_2023.shp\")\n",
    "medium_grid= os.path.join(annotation_path, \"2023 test sample grid/Medium_desnity_test_sample_grid_2023.shp\")\n",
    "high_grid = os.path.join(annotation_path, \"2023 test sample grid/High_desnity_test_sample_grid_2023.shp\")\n",
    "veryhigh_grid = os.path.join(annotation_path, \"2023 test sample grid/Very_high_desnity_test_sample_grid_2023.shp\")\n",
    "\n",
    "low_annotation = os.path.join(annotation_path, \"2023 test samples/Low_density_test_samples_2023.shp\")\n",
    "medium_annotation = os.path.join(annotation_path, \"2023 test samples/Medium_density_test_samples_2023.shp\")\n",
    "high_annotation = os.path.join(annotation_path, \"2023 test samples/High_density_test_samples_2023.shp\")\n",
    "veryhigh_annotation = os.path.join(annotation_path, \"2023 test samples/Very_high_density_test_samples_2023.shp\")\n",
    "# cluster_size = 9\n",
    "\n",
    "item= [\"low\", \"medium\",\"high\", \"veryhigh\" ]\n",
    "grid_path=[low_grid, medium_grid, high_grid, veryhigh_grid]\n",
    "label_path=[low_annotation, medium_annotation, high_annotation, veryhigh_annotation]\n",
    "\n",
    "\n",
    "define_crs = 'EPSG:32736'\n",
    "\n",
    "# search_distance = 0.84\n",
    "output_accuracy_path = os.path.join(annotation_path, 'paper_test.csv')\n",
    "\n",
    "\n",
    "Folder = \"/home/zijing/wildebeest\"\n",
    "WEIGHT_PATH = os.path.join(Folder,'tmp/checkpoint/weights')\n",
    "cluster_size = 16\n",
    "\n",
    "\n",
    "nfold = 5\n",
    "\n",
    "NUM = 2\n",
    "PATCH_SIZE = 336\n",
    "TILE_MAX_SIZE = PATCH_SIZE * NUM\n",
    "\n",
    "INPUT_BANDS = [0,1,2]\n",
    "NUMBER_BANDS=len(INPUT_BANDS)\n",
    "\n",
    "\n",
    "CONTRAST = False\n",
    "fold_nums = 5\n",
    "\n",
    "weight_set = 0.8\n",
    "lr_set = 0.0001\n",
    "\n",
    "model = unet(pretrained_weights=None, input_size=(PATCH_SIZE,PATCH_SIZE,NUMBER_BANDS), lr = lr_set, regularizers = regularizers.l2(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run prediction on test dataset and save the prediction as points\n",
    "#The test images are stored separately in the folder for each class\n",
    "#The predictions will be saved in the corresponding folders (see below Final_Output_dir)\n",
    "\n",
    "for i in range(len(item)): \n",
    "\n",
    "    grids = grid_path[i]\n",
    "    gt = label_path[i]\n",
    "    to_eval = item[i]\n",
    "    print(\"Class to be evaluated: \", to_eval)\n",
    "    \n",
    "    tiled_folder = os.path.join(annotation_path, \"test_images/{}/image\".format(to_eval))\n",
    "    Output_dir = os.path.join(annotation_path, \"test_images/{}/predict_test\".format(to_eval))\n",
    "    Final_Output_dir = os.path.join(annotation_path, \"test_images/{}/predict_test_merge\".format(to_eval))\n",
    "\n",
    "    predict_file_name = '{}_merge.shp'.format(to_eval)\n",
    "\n",
    "    target_images = get_images_to_predict(tiled_folder)\n",
    " \n",
    "    if not os.path.exists(Output_dir):\n",
    "        os.makedirs(Output_dir)\n",
    "                \n",
    "    for ti in target_images:\n",
    "        print(ti)\n",
    "        f = ti\n",
    "        file_name = os.path.split(f)[1]\n",
    "        out_name, file_extension = os.path.splitext(file_name)\n",
    "        print(out_name)\n",
    "        shp_path = os.path.join(Output_dir, out_name+'.shp')\n",
    "        mask_path = os.path.join(Output_dir, out_name+'.tif')\n",
    "    \n",
    "        if Path(shp_path).is_file() == True:\n",
    "          print(f\"Prediction already exists. Skip.\")\n",
    "          continue\n",
    "    \n",
    "        with rasterio.open(f) as src:\n",
    "    \n",
    "            detectedMask = detect_wildebeest(model, WEIGHT_PATH, src, width=PATCH_SIZE, height=PATCH_SIZE, stride = 256,\n",
    "                                batch_size=12, stretch=CONTRAST, num_folds=nfold) # WIDTH and HEIGHT should be the same and in this case Stride is 50 % width\n",
    "            # visualize_prediction(detectedMask)\n",
    "            #Write the mask to file\n",
    "            # visualize_data(np.moveaxis(np.uint8(src.read()), 0,-1),np.expand_dims(detectedMask, axis=2))\n",
    "            writeResultsToDisk(detectedMask, src, src.meta['transform'], shp_path, None, cluster_size)\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "\n",
    "    if not os.path.exists(Final_Output_dir):\n",
    "        os.makedirs(Final_Output_dir)\n",
    "    \n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(Output_dir, topdown=False):\n",
    "        for name in files:\n",
    "          out_name, file_extension = os.path.splitext(name)\n",
    "          if file_extension == '.shp':\n",
    "            print(name)              \n",
    "            points = gpd.read_file(os.path.join(root,name))      \n",
    "            file_list.append(points)\n",
    "    rdf = gpd.pd.concat(file_list, ignore_index=True)     \n",
    "    rdf.to_file(os.path.join(Final_Output_dir, predict_file_name))\n",
    "    print(rdf.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "pixel wise comparison and accuracy evaluation\n",
    "\n",
    "a predicted point will be considered correct if it falls within the seaching radius (unit: number of pixels) around the ground truth point\n",
    "\"\"\"\n",
    "\n",
    "cluster_size_list = [16]\n",
    "search_distance_list = [3]\n",
    "\n",
    "define_crs = 'EPSG:32736'\n",
    "\n",
    "\n",
    "Total_TP = 0\n",
    "Total_FP = 0\n",
    "Total_FN = 0\n",
    "\n",
    "\n",
    "df_list = []\n",
    "wrong_detection_list = []\n",
    "\n",
    "for cluster_size in cluster_size_list:\n",
    "    print(\"Cluster size: \", cluster_size)\n",
    "    \n",
    "    for s in search_distance_list:\n",
    "        print(\"Search distance: \", s)\n",
    "        \n",
    "        all_Total_TP = 0\n",
    "        all_Total_FP = 0\n",
    "        all_Total_FN = 0\n",
    "        \n",
    "        accuracy_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(item)):\n",
    "                      \n",
    "            grids = grid_path[i]\n",
    "            gt = label_path[i]\n",
    "            to_eval = item[i]\n",
    "            print(\"Class to be evaluated: \", to_eval)\n",
    "\n",
    "            if os.path.exists(grids) != True:\n",
    "                print(f\"{grids} does not exist.\")\n",
    "\n",
    "            # iterate over the smample plots\n",
    "            roi = gpd.read_file(grids)\n",
    "            print(roi.crs)\n",
    "\n",
    "            if roi.crs is None:\n",
    "                roi = roi.set_crs(define_crs)\n",
    "            roi = roi[~roi.is_empty] # remove potential empty polygons\n",
    "            print(\"found {} valid geometries\".format(len(roi)))\n",
    "    #             print(\"Type of roi: \", type(roi))\n",
    "    #             print(roi)\n",
    "            Total_TP = 0\n",
    "            Total_FP = 0\n",
    "            Total_FN = 0\n",
    "            \n",
    "#             out_path = os.path.join(annotation_path, 'test_images/{}/predict_combine_{}'.format(to_eval, cluster_size))\n",
    "            out_path = os.path.join(annotation_path, 'test_images/{}/predict_test_merge'.format(to_eval))\n",
    "    \n",
    "            if os.path.exists(out_path) == False:\n",
    "                print(\"Prediction point path does not exists!\")\n",
    "#             out_name = '{}_cluster_{}'.format(to_eval, cluster_size)\n",
    "            out_name = '{}_merge'.format(to_eval)\n",
    "\n",
    "            shp_path = os.path.join(out_path, out_name+'.shp') \n",
    "            \n",
    "            \n",
    "            for index in range(len(roi)):\n",
    "                print(index)\n",
    "#                 seg_map = os.path.join(seg_map_path, \"image_{:0>6d}.tif\".format(index))\n",
    "                img = os.path.join(annotation_path, \"test_images/\"+to_eval+'/image', \"image_{:0>6d}.tif\".format(index))\n",
    "                with rasterio.open(img) as src:\n",
    "                    bou = src.bounds\n",
    "                    true_pts = gpd.read_file(gt)\n",
    "                    predict_pts = gpd.read_file(shp_path)\n",
    "                    print(true_pts.crs)\n",
    "                    print(src.crs)\n",
    "                    if true_pts.crs == src.crs and predict_pts.crs == src.crs:\n",
    "                        print('The crs of the points are the same as the crs from the raster image!')\n",
    "                        bou = src.bounds\n",
    "                        true_pts = gpd.read_file(gt, bbox = bou, encoding='utf-8')\n",
    "                        predict_pts = gpd.read_file(shp_path, bbox = bou, encoding='utf-8')\n",
    "              \n",
    "                    else:    \n",
    "                        print('The crs of the points are different from the crs from the raster image!')\n",
    "                        true_pts = gpd.read_file(gt, encoding='utf-8').to_crs(src.crs)\n",
    "                        predict_pts = gpd.read_file(shp_path, encoding='utf-8').to_crs(src.crs)\n",
    "                        \n",
    "                        roi = roi.to_crs(src.crs)\n",
    "#                         df_bou = gpd.GeoDataFrame(gpd.GeoSeries(bou), columns=['geometry'])\n",
    "                        true_pts = gpd.sjoin(true_pts, roi[roi.index==index], op='within', how='inner')\n",
    "                        predict_pts = gpd.sjoin(predict_pts, roi[roi.index==index], op='within', how='inner')\n",
    "                        \n",
    "                    print(len(true_pts.index))\n",
    "                    print(len(predict_pts.index))\n",
    "                    true_pts_x, true_pts_y = CrsToPixel(true_pts.geometry, src)\n",
    "                    predict_pts_x, predict_pts_y = CrsToPixel(predict_pts.geometry, src)               \n",
    "                    \n",
    "                accuracy = evaluation_pixel(list(zip(true_pts_x,true_pts_y)), list(zip(predict_pts_x,predict_pts_y)), threshold=s)\n",
    "                print(\"Accuracy of grid {}\".format(index), accuracy)\n",
    "                if accuracy['FP'] > 10 or accuracy['FN'] > 10:\n",
    "                    accuracy[\"index\"] = index\n",
    "                    accuracy[\"category\"] = to_eval\n",
    "                    accuracy[\"search_distance\"] = s\n",
    "                    wrong_detection_list.append(accuracy)\n",
    "                '''    \n",
    "                if accuracy['FP'] > 10 or accuracy['FN'] > 10:\n",
    "                    fig, ax = plt.subplots(1,3, sharex=True, sharey=True)\n",
    "                    ax[0].set_title(\"Satellite image\")\n",
    "                    ax[1].set_title(\"Ground truth\")\n",
    "                    ax[2].set_title(\"Detection\")\n",
    "                    data = cv2.imread(img)\n",
    "                    ax[0].imshow(data)\n",
    "                    ax[1].imshow(data)\n",
    "                    ax[1].scatter(true_pts_y, true_pts_x, color='red')\n",
    "                    ax[2].imshow(data)\n",
    "                    ax[2].scatter(predict_pts_y, predict_pts_x, color='blue')                    \n",
    "                    plt.show()\n",
    "                '''        \n",
    "                Total_TP += accuracy['TP']\n",
    "                Total_FP += accuracy['FP']\n",
    "                Total_FN += accuracy['FN']\n",
    "\n",
    "        #         print(len(total_pts))\n",
    "\n",
    "            Total_precision = Total_TP/(Total_TP+Total_FP+epsilon)\n",
    "            Total_recall = Total_TP/(Total_TP+Total_FN+epsilon)\n",
    "            Total_f1 = 2*(Total_precision*Total_recall)/(Total_precision+Total_recall+epsilon)\n",
    "        #         print(\"Total Precision: \", Total_precision)\n",
    "        #         print(\"Total Recall: \", Total_recall)\n",
    "        #         print(\"Total F1-score: \", Total_f1)\n",
    "            total_accuracy = {\n",
    "              \"TP\": Total_TP,\n",
    "              \"FP\": Total_FP,\n",
    "              \"FN\": Total_FN,\n",
    "              \"Precision\":Total_precision,\n",
    "              \"Recall\":Total_recall,\n",
    "              \"F1\":Total_f1\n",
    "          }   \n",
    "\n",
    "            print(\"Accuracy of class {}\".format(to_eval), total_accuracy)\n",
    "            accuracy_list.append(total_accuracy) \n",
    "            \n",
    "            all_Total_TP += total_accuracy['TP']\n",
    "            all_Total_FP += total_accuracy['FP']\n",
    "            all_Total_FN += total_accuracy['FN']\n",
    "\n",
    "        all_Total_precision = all_Total_TP/(all_Total_TP+all_Total_FP+epsilon)\n",
    "        all_Total_recall = all_Total_TP/(all_Total_TP+all_Total_FN+epsilon)\n",
    "        all_Total_f1 = 2*(all_Total_precision*all_Total_recall)/(all_Total_precision+all_Total_recall+epsilon)\n",
    "\n",
    "\n",
    "        all_accuracy = {\n",
    "              \"TP\": all_Total_TP,\n",
    "              \"FP\": all_Total_FP,\n",
    "              \"FN\": all_Total_FN,\n",
    "              \"Precision\":all_Total_precision,\n",
    "              \"Recall\":all_Total_recall,\n",
    "              \"F1\":all_Total_f1\n",
    "          }\n",
    "        print(\"Accuracy of all classes together: \", all_accuracy)\n",
    "        item.append(\"total\")\n",
    "        accuracy_list.append(all_accuracy)\n",
    "#         print(accuracy_list)            \n",
    "\n",
    "        df = pd.DataFrame(data=accuracy_list)\n",
    "#         print(df)\n",
    "#         print(item)\n",
    "        df['new_index'] = item\n",
    "    #     df.set_index('new_index')\n",
    "        df['cluster size'] = cluster_size\n",
    "        df['search distance'] = s\n",
    "#         print(df)\n",
    "#         df.to_csv(os.path.join(annotation_path, 'accuracy2023_cluster_{}_search_pixel{}.csv'.format(cluster_size, s)), index = False)\n",
    "\n",
    "        item.pop()\n",
    "        df_list.append(df)\n",
    "\n",
    "all_df = pd.concat(df_list)\n",
    "print(all_df)\n",
    "all_df.to_csv(os.path.join(annotation_path, 'Final_accuracy_test.csv'), index = False)\n",
    "\n",
    "wrong_detection = pd.DataFrame(data=wrong_detection_list)\n",
    "wrong_detection.to_csv(os.path.join(annotation_path, 'Final_accuracy_test_wrong.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
